## PROJECT CONTEXT: 

### **Statistics for pairwise interaction (SPIs) in multivariate time series (MTS)**
-A multivariate time series (MTS) can capture the co-evolution of any physical process, from the brightness of celestial objects , to the movement dynamics of microscopic organisms. Uncovering similar dynamical properties between such vastly different systems can substantially progress scientific understanding. For example, our understanding of the spontaneous synchronization that emerges in diverse physical systems—from flashing fireflies, to Josephson junctions, or social networks—developed by observing and analyzing similar dynamics in coupled phase oscillators, such as the mid-century models of Kuramoto and Winfree. 

While it is now well-known that certain dynamical properties—e.g., synchronization, long-range correlations, and cascading failures—emerge across many spatiotemporal scales of complex dynamical systems, there is still no systematic way to quantify, from data, the similarity between systems that exhibit these properties. These similarities are often qualitatively referred to as \textit{emergent properties} which arise from the interactive between co-evolving processes. Paradigmatic examples of dynamical properties emerging from the interaction of processes include: groups of fireflies suddenly flashing in unison because individuals synchronize with others in their visual field; financial markets crashing due to hidden interactions between asset pairs; and rolling blackouts occurring in electrical grids due to interconnected transmission lines. Although emergence has been established to be a consequence of interactions, it remains elusive as to how we can analyze these interactions in order to quantify similarities in the dynamics of diverse physical processes. Recent work has demonstrated a way of representing univariate time series in a way that puts diverse time series (of different sampling rates and lengths) in a common space, in which dynamics generated from similar systems are close in the space. This involves representing each time series in terms of a set of extracted real-valued ‘features’, that capture different types of information about dynamical structure, ranging from simple distributional statistics (such as mean and variance) to advanced nonlinear time-series characteristics (such as the correlation dimension and auto-mutual information) [fulcher2013]. Since the diverse feature-vector representation is powerful way to organize, classify, and cluster collections of univariate time series, adapting this approach to distributed dynamical systems that are each measured as a MTS, could form the basis for powerful new ways to similarly perform statistical learning on MTS. However, devising an appropriate feature-based representation of such data is far more challenging since the types of relevant properties of MTS are vast, from the fundamental level of individual pairs (quantifying how pairs of system elements interact) to the whole systems-level (quantifying the emergent dynamical patterns directly). One common approach to quantifying the similarity between MTS is to focus on the strength of pairwise dependencies across the system (e.g., ‘functional connectivity’ as a network of Pearson correlations is common in neuroimaging), under the assumption that differences in interactions at the level of specific pairs characterize differences between different types of distributed dynamical processes. However, as different types of processes will in general be characterized by different types of interactions, this approach requires manually selecting an appropriate statistic of pairwise interaction (SPI), that captures the relevant interactions. Furthermore, methods for quantifying and understanding the relationship between pairs of time series are vast, ranging from simple summary statistics (like Pearson correlation) to advanced multi-step causality analysis (like convergent cross-mapping), with each varying in computational strain, making the selection far from simple. A forward-looking approach for tackling this selection problem was outlined in Cliff et al.'s recent work, which unified 250 different SPIs, providing a systematic way to simultaneously leverage diverse scientific methods for quantifying relationships in MTS. 

Cliff's approach is powerful for tailoring scientific methods to the needs of a given statistical learning problem involving MTS, allowing researchers to select (or combine sets of) the most appropriate SPIs for a given problem in a data-driven way. However, comparing a collection of $M$-channel MTS in terms of their pairwise dependencies (yielding an $M \times M$ adjacency matrix for each $M \times T$ MTS per statistic) is only possible for systems with the same number of elements ($M$). And since these comparisons capture how specific system elements relate to other specific elements, we therefore require a common set of elements to be defined across a multivariate time series, which in turn narrows the scope of the method's applicability for diverse statistical learning. Furthermore, similarities within (and differences between) general classes of dynamical processes are characterized by the patterns of their interactions, rather than the strength of interactions between specific pairs (as captured by the $M \times M$ pairwise dependency representation of MTS). Thus, comparing sets of pairwise dependencies between system elements directly is not an appropriate general representation for organizing MTS according to the similarity of their underlying dynamical processes. 

Here, we introduce a feature-based representation of MTS that quantitatively captures differences in general equivalence classes of dynamical processes and is applicable to diverse systems of varying sampling rates, sampling periods, and system sizes. Our approach leverages different methods for quantifying pairwise interactions between system elements, but instead of representing each MTS directly in terms of the strength of interactions between individual pairs, our approach captures the similarities and differences between the results of applying different SPIs to a given MTS. 

For example, consider applying three SPIs - Pearson correlation, Spearman correlation, and Transfer Entropy - to a collection of MTS, yielding three pairwise dependencies for each channel (element) within its parent MTS. Our approach operates in the SPI-SPI feature space, extracting features from each MTS as the set of three (rank-based?) correlations between the output of three SPIs across the system elements: 
- (i) Pearson and Spearman; 
- (ii) Pearson and Transfer Entropy; and 
- (iii) Spearman and Transfer Entropy. 

Crucially, because the resulting feature vector is always the same dimension ($N \choose 2$ for $N$ chosen SPI features), regardless of the dimensionality $M$ of each individual system, this approach puts MTS of different sizes and sampling rates in a common space. Importantly, this permits diverse MTS, varying in dimension, to be analyzed using traditional statistical learning algorithms. And because it looks for patterns of similarities between interactions across the full set of system elements, it is able to capture general properties of interactions in a system in a way that is not sensitive to specific pairs of elements. 

In this work, we introduce and formalise this approach for quantifying MTS, and implement it at scale, across a large collection of ($>1000$) MTS and combining similarities between a large collection of ($>250$) SPIs. We show that this approach provides a robust signature that captures differences between general classes of distributed dynamical processes directly from MTS data. After motivating our approach, we introduce model simulations of different classes of complex dynamics, validating that our approach successfully organizes MTS according to the underlying class or generative dynamics across a broad range of system sizes and scales. Then, in a novel demonstration, we use our method to organize a large database of diverse dynamical system MTS data (ranging from <X> to <Y> to <Z>) within a common space, clustering groups of processes based on the closeness of their hidden, underlying dynamics, which may otherwise be undetected, or hidden to either the human eye or computer vision algorithms (see Fig. <not done yet>). Our analysis uncovers striking similarities between diverse classes of synthetic models and real-world systems, ranging from across the theoretical and empirical sciences.

### **Extracting informative features from multivariate time series.** 

**Basic intuitive introduction**
 - For certain spatiotemporal dependency structures in multivariate time series (MTS), some statistics for pairwise interactions (SPIs) yield similar values to one another; for other dependency structures structures, these same statistics will yield very different values to one another. Therefore, by characterising the degree to which a comprehensive (and selective) set of SPIs is similar, we can in-turn capture a large number of dynamical properties of a multivariate time series by considering the intersection and exposition of the underlying statistical assumptions.
 - We can develop the intuition by first considering two distinct classes of systems: Gaussian-distributed noise and Cauchy-distributed noise. The \textit{character} of the relationship between the time series can be evaluated using two of the most broadly used and well-understood statistics for measuring pairwise relationships (SPIs): Pearson's product-moment correlation coefficient (Pearson's $r$), and Spearman's rank-correlation coefficient (Spearman's $\rho$ or $r_s$). 
 - Suppose we have two time series generated by sampling a standard Gaussian distribution, forming a dual-channel MTS. Were we to use \textit{either} Pearson's $r$ or Spearman's $\rho$ to compute the empirical similarity between these two time series, we would obtain near-identical results. This is because the assumptions underlying both statistics - namely linearity for Pearson's $r$ and monotonicity for Spearman's $\rho$ - are upheld for applications to Gaussian-distributed data. Contrastingly, if we were to instead generate two time series, this time sampled from a Cauchy-distributed process, and then compute the similarity using these same two statistics, we would obtain markedly different results. This is because Pearson's $r$ draws information from the raw values, whereas Spearman's $\rho$ uses only the ranks of the data point, significantly dampening the impact of extreme, non-Gaussian outliers. 
 - Thus, we could conclude that the degree to which Spearman's $\rho$ and Pearson's $r$ diverges infers the degree to which the relationship's monotonicity is preserved while its linearity is distorted. Therefore, this signal (the absolute divergence $\abs{\rho - r}$) is a key signature of the underlying generative process of our observed system to be Cauchy-like as opposed to Gaussian. 
 - While this intuition functions well for contrived examples, like our Gaussian-Cauchy noise systems, the broader notion of distinguishing between two (or more) types of dynamics is not so straightforward. 
 - In general, there are many potential spatiotemporal dependencies that may distinguish one MTS from another, and particular dynamical properties may only emerge (or be supressed) due to an underlying dependency structure, such as the outlier distribution in our Gaussian-Cauchy example. 
 - Fortunately, there are also many hundreds of existing techniques that have been developed to measure diverse relationships between pairs of time series (referred to in this work as SPIs). These dependency structures may be lagged in time (cross-correlation) or even misaligned (dynamic time warping); other dependencies may emerge due to the causality attributable to the directed flow of information in a linear, Gaussian environment (Granger causality) or can even emerge from capturing nonlinearity (transfer entropy).